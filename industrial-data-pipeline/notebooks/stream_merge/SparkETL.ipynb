{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695d2a00-6173-414c-94c5-9408a43f806e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to write to StarRocks: An error occurred while calling o190.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 14) (172.18.0.11 executor 0): java.sql.BatchUpdateException: Failed to load data into tablet 66888, because of too many versions, current/limit: 1001/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1: BE:10001\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:816)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:418)\n",
      "\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.sql.SQLException: Failed to load data into tablet 66888, because of too many versions, current/limit: 1001/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1: BE:10001\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:130)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1061)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:795)\n",
      "\t... 16 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.sql.BatchUpdateException: Failed to load data into tablet 66888, because of too many versions, current/limit: 1001/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1: BE:10001\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:816)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:418)\n",
      "\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.sql.SQLException: Failed to load data into tablet 66888, because of too many versions, current/limit: 1001/1000. You can reduce the loading job concurrency, or increase loading data batch size. If you are loading data with Routine Load, you can increase FE configs routine_load_task_consume_second and max_routine_load_batch_size,:  be:127.0.0.1: BE:10001\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:130)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1061)\n",
      "\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:795)\n",
      "\t... 16 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, unix_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeToMachineMetrics\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# JDBC configs\n",
    "jdbc_url = \"jdbc:mysql://starrocks-allin1:9030/industrial_warehouse\"\n",
    "jdbc_props = {\"user\": \"root\", \"password\": \"\", \"driver\": \"com.mysql.cj.jdbc.Driver\"}\n",
    "\n",
    "# Load staging and dimension tables\n",
    "iot_df = spark.read.jdbc(jdbc_url, \"staging_iot_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "mes_df = spark.read.jdbc(jdbc_url, \"staging_mes_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "scada_df = spark.read.jdbc(jdbc_url, \"staging_scada_stream\", properties=jdbc_props).withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "status_df = spark.read.jdbc(jdbc_url, \"MachineStatus\", properties=jdbc_props)\n",
    "alarm_df = spark.read.jdbc(jdbc_url, \"AlarmCodes\", properties=jdbc_props)\n",
    "\n",
    "# Best MES within ¬±1 hour\n",
    "mes_join = iot_df.alias(\"i\").join(mes_df.alias(\"m\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"m.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 3600)\n",
    "\n",
    "mes_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "mes_best = mes_join.withColumn(\"rn\", row_number().over(mes_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"i.Temperature_C\"),\n",
    "        col(\"i.Vibration_mm_s\"),\n",
    "        col(\"i.Pressure_bar\"),\n",
    "        col(\"m.Operator_ID\"),\n",
    "        col(\"m.Units_Produced\"),\n",
    "        col(\"m.Defective_Units\"),\n",
    "        col(\"m.Production_Time_min\")\n",
    "    )\n",
    "\n",
    "# Best SCADA within ¬±15 minutes\n",
    "scada_join = iot_df.alias(\"i\").join(scada_df.alias(\"s\"), \"Machine_ID\") \\\n",
    "    .withColumn(\"time_diff\", abs(unix_timestamp(col(\"i.Timestamp\")) - unix_timestamp(col(\"s.Timestamp\")))) \\\n",
    "    .filter(col(\"time_diff\") <= 900)\n",
    "\n",
    "scada_window = Window.partitionBy(\"i.Machine_ID\", \"i.Timestamp\").orderBy(\"time_diff\")\n",
    "scada_best = scada_join.withColumn(\"rn\", row_number().over(scada_window)).filter(col(\"rn\") == 1) \\\n",
    "    .select(\n",
    "        col(\"i.Timestamp\").alias(\"iot_Timestamp\"),\n",
    "        col(\"i.Machine_ID\"),\n",
    "        col(\"s.Power_Consumption_kW\"),\n",
    "        col(\"s.Machine_Status\").alias(\"Machine_Status_Name\"),\n",
    "        when(col(\"s.Alarm_Code\") == \"\", \"None\").otherwise(col(\"s.Alarm_Code\")).alias(\"Alarm_Code_Desc\")\n",
    "    )\n",
    "\n",
    "# Join all and enrich with IDs\n",
    "final_df = mes_best.alias(\"m\").join(scada_best.alias(\"s\"),\n",
    "    (col(\"m.Machine_ID\") == col(\"s.Machine_ID\")) & (col(\"m.iot_Timestamp\") == col(\"s.iot_Timestamp\")),\n",
    "    how=\"left\"\n",
    ").join(status_df.alias(\"ms\"), col(\"s.Machine_Status_Name\") == col(\"ms.StatusName\"), \"left\") \\\n",
    " .join(alarm_df.alias(\"ac\"), col(\"s.Alarm_Code_Desc\") == col(\"ac.AlarmDescription\"), \"left\") \\\n",
    " .select(\n",
    "    col(\"m.iot_Timestamp\").alias(\"Timestamp\"),\n",
    "    col(\"m.Machine_ID\"),\n",
    "    col(\"m.Temperature_C\"),\n",
    "    col(\"m.Vibration_mm_s\"),\n",
    "    col(\"m.Pressure_bar\"),\n",
    "    col(\"m.Operator_ID\"),\n",
    "    col(\"m.Units_Produced\"),\n",
    "    col(\"m.Defective_Units\"),\n",
    "    col(\"m.Production_Time_min\"),\n",
    "    col(\"s.Power_Consumption_kW\"),\n",
    "    col(\"ms.StatusID\").alias(\"Machine_Status_ID\"),\n",
    "    col(\"ms.StatusName\").alias(\"Machine_Status\"),\n",
    "    col(\"ac.AlarmID\").alias(\"Alarm_ID\"),\n",
    "    col(\"ac.AlarmDescription\").alias(\"Alarm_Code\")\n",
    ")\n",
    "\n",
    "# Optional: Write to StarRocks\n",
    "try:\n",
    "    final_df.write.mode(\"append\").format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"machine_metrics\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .save()\n",
    "    print(\"‚úÖ Data inserted into machine_metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to write to StarRocks:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d50569-1252-4a12-80db-37a7c9cf67c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
